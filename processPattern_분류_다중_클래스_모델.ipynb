{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6GEQzgbaT6YZDbDHv23WO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leejunho12316/P-Project-Machine-Learning/blob/main/processPattern_%EB%B6%84%EB%A5%98_%EB%8B%A4%EC%A4%91_%ED%81%B4%EB%9E%98%EC%8A%A4_%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시작\n",
        "패키지 install, 함수 정의"
      ],
      "metadata": {
        "id": "OJPSTDzyzIhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/192931\n"
      ],
      "metadata": {
        "id": "VkU_XpyovwZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jpype1"
      ],
      "metadata": {
        "id": "b-JVCazvsFYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90da868-afb2-4f22-9e0e-38760755a4d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jpype1\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/465.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/465.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from jpype1) (23.2)\n",
            "Installing collected packages: jpype1\n",
            "Successfully installed jpype1-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "OgiAX981sG0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5750110-9a66-4987-9b77-993b27daa4f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: konlpy\n",
            "Successfully installed konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "EL4wlS_mtOrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014612e6-80f6-412a-e3d4-148f3ce67000"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한글파일 unzip시 깨짐 encoding decoding으로 방지하는 함수, num개만 압축해제\n",
        "def unzip(source_path, dest_path):\n",
        "  with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "    zipInfo = zf.infolist()\n",
        "    for member in zipInfo:\n",
        "      try:\n",
        "        #print(member.filename.encode('cp437').decode('euc-kr','ignore'))\n",
        "        member.filename = member.filename.encode('cp437').decode('euc-kr','ignore')\n",
        "        zf.extract(member,dest_path)\n",
        "      except:\n",
        "        print(source_path)\n",
        "        raise Exception('??')"
      ],
      "metadata": {
        "id": "XbnSxMtmE93w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비\n",
        "데이터 다운로드"
      ],
      "metadata": {
        "id": "yRCeHJvpAavu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OZeSzFer91zV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "source_path = '/content/drive/MyDrive/ML_Data/146.낚시성 기사 탐지 데이터.zip'\n",
        "destination_path = '/content/146.낚시성 기사 탐지 데이터.zip'\n",
        "shutil.copyfile(source_path,destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MCgltYRr94D1",
        "outputId": "55f99bfe-5d60-4162-8d5f-e72fdeb4bfbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/146.낚시성 기사 탐지 데이터.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data 다운로드/압축해제\n",
        "원문 Clickbait Direct 만 뽑아 새로운 데이터셋 생성 - 본문/제목 다중입력"
      ],
      "metadata": {
        "id": "SbbbErUHyu2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 저장을 위한 경로 만들기\n",
        "root_path = '/content/root'\n",
        "training_data_path = os.path.join(root_path,'Training')\n",
        "test_data_path = os.path.join(root_path,'Test')\n",
        "\n",
        "p = [root_path,training_data_path,test_data_path]\n",
        "for i in p:\n",
        "  if not os.path.exists(i):\n",
        "    os.mkdir(i)"
      ],
      "metadata": {
        "id": "W4Q4xqlgFYag"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 중 Clickbait_Direct 아닌거 삭제 - Clickbait_Direct 만 processPattern 가짐\n",
        "unzip('/content/146.낚시성 기사 탐지 데이터.zip','/content')\n",
        "raw_data_path = '/content/146.낚시성 기사 탐지 데이터'\n",
        "\n",
        "for root,dirs,files in os.walk(raw_data_path):\n",
        "  for fn in files:\n",
        "    if 'Clickbait_Direct' not in fn:\n",
        "      os.remove(root+'/'+fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "uguErh06BvaU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(임시) zip 하나만 풀어서 dataset 만들기\n",
        "a='/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_EC.zip'\n",
        "b='/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_IS.zip'\n",
        "unzip(a,training_data_path)\n",
        "unzip(b,test_data_path)"
      ],
      "metadata": {
        "id": "G8lCSqPEin6o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "aG8_NSJry4LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문->토큰화->벡터화(원-핫 인코딩)\n",
        "\n",
        "1.   train_data\n",
        "[본문.........],[본문..........]....\n",
        "2.   token_train_data\n",
        "[시퀀스시퀀스시퀀스],[시퀀스시퀀스시퀀스],[시퀀스시퀀스시퀀스]....\n",
        "3. x_train\n",
        "[0,0,0,0,1,0,0,,,,],[0,1,0,0,0,0,,,,,,],,,,,\n",
        "\n"
      ],
      "metadata": {
        "id": "OG5sgl_7jcPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터준비"
      ],
      "metadata": {
        "id": "TV3i624EAIHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "#training_data 경로의 모든 파일 읽어오기 (json)\n",
        "training_files = []\n",
        "for root, dirs, files in os.walk(training_data_path):\n",
        "  for fn in files:\n",
        "    training_files.append(root+'/'+fn)\n",
        "\n",
        "\n",
        "#각 json 파일의 본문을 리스트에 추가\n",
        "train_data_title = []#np.array([])\n",
        "train_data = []#np.array([])\n",
        "train_labels = []\n",
        "\n",
        "for i in training_files:\n",
        "  with open(i,'r',encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    #제목\n",
        "    newsTitle = data.get('sourceDataInfo').get('newsTitle')\n",
        "    newsTitle = re.sub('[()]',' ',newsTitle)\n",
        "    newsTitle = re.sub(r'\\\\',' ',newsTitle)\n",
        "    train_data_title.append(newsTitle) #train_data_title = np.append(newsTitle, train_data_title)\n",
        "    #본문\n",
        "    newsContent = data.get('sourceDataInfo').get('newsContent')\n",
        "    #newsContent = re.sub(r'\\n',' ', newsContent) # 엔터키를 없애버리면 sentencepiece 패키지가 멍청해서 인식을 못함\n",
        "    newsContent = re.sub('[()]',' ',newsContent)\n",
        "    newsContent = re.sub(r'\\\\',' ',newsContent)\n",
        "    train_data.append(newsContent) #train_data = np.append(newsContent,train_data)\n",
        "    #라벨\n",
        "    train_labels.append(int(data.get('sourceDataInfo').get('processPattern')))\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "#위에랑 똑같은거 test에도\n",
        "test_files = []\n",
        "for root,dirs,files in os.walk(test_data_path):\n",
        "  for fn in files:\n",
        "    test_files.append(root+'/'+fn)\n",
        "\n",
        "test_data = []#np.array([])\n",
        "test_data_title = []#np.array([])\n",
        "test_labels = []\n",
        "\n",
        "for i in test_files:\n",
        "  with open(i,'r',encoding = 'utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    #제목\n",
        "    newsTitle = data.get('sourceDataInfo').get('newsTitle')\n",
        "    newsTitle = re.sub('[()]',' ',newsTitle)\n",
        "    newsTitle = re.sub(r'\\\\',' ',newsTitle)\n",
        "    test_data_title.append(newsTitle) #test_data_title = np.append(newsTitle, test_data_title)\n",
        "\n",
        "    newsContent = data.get('sourceDataInfo').get('newsContent')\n",
        "    newsContent = re.sub('[()]',' ',newsContent)\n",
        "    newsContent = re.sub(r'\\\\',' ',newsContent)\n",
        "    test_data.append(newsContent) #test_data = np.append(newsContent,test_data)\n",
        "\n",
        "    test_labels.append(int(data.get('sourceDataInfo').get('processPattern')))"
      ],
      "metadata": {
        "id": "vHDKQUMtkcZ6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문 - 토큰화"
      ],
      "metadata": {
        "id": "G62H-jtO-E3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 토큰화 위한 과정\n",
        "#https://velog.io/@lighthouse97/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%A0%84%EC%B2%98%EB%A6%AC-%ED%86%A0%ED%81%B0%ED%99%94\n",
        "\n",
        "#토큰화 - 1 형태소 분석\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "text = '아이고 배고파 ㅠㅠ 이럴수가. 컵라면을 사먹어요'\n",
        "\n",
        "morphs = okt.morphs(text)\n",
        "ps = okt.pos(text)\n",
        "noun = okt.nouns(text)\n",
        "print(f\"형태소 분석 : {morphs}\")\n",
        "print(f'품사 태깅 : {ps}')\n",
        "print(f'명사 추출 : {noun}')"
      ],
      "metadata": {
        "id": "c7e1Hb5rr2kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd7e75c-9cbb-4ef4-90ee-cefe402c57ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 분석 : ['아이고', '배고파', 'ㅠㅠ', '이럴수가', '.', '컵라면', '을', '사먹어요']\n",
            "품사 태깅 : [('아이고', 'Exclamation'), ('배고파', 'Adjective'), ('ㅠㅠ', 'KoreanParticle'), ('이럴수가', 'Adjective'), ('.', 'Punctuation'), ('컵라면', 'Noun'), ('을', 'Josa'), ('사먹어요', 'Verb')]\n",
            "명사 추출 : ['컵라면']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화 - 2 Subword Tokenizer\n",
        "#BPE 알고리즘을 구현한 구글의 Sentencepiece를 사용해 내부 단어 분리 진행\n",
        "\n",
        "#이름.model 이 sentencepiece에서 사용되는 단어집합\n",
        "#이름.vocal 이 단어 집합을 텍스트 파일로 저장한것\n",
        "#모든 텍스트파일을 하나의 txt에 쳐넣고 학습해 model을 만들고 각각의 train_data 안에 있는 데이터에 sp.encode를 해주면 된다.\n",
        "\n",
        "import sentencepiece as spm\n",
        "#모든 텍스트 하나의 txt에 쳐넣어 sentencepiece 학습시키기\n",
        "with open('/content/test1.txt','w',encoding='utf-8') as f:\n",
        "  for i in range(0,len(train_data)):\n",
        "    f.write(train_data[i])\n",
        "  for i in range(0,len(test_data)):\n",
        "    f.write(test_data[i])\n",
        "\n",
        "spm.SentencePieceTrainer.Train('--input=test1.txt --model_prefix=mine --vocab_size=30000 --model_type=bpe --max_sentence_length=9999')\n",
        "\n",
        "#Sentence piece 객체 생성\n",
        "sp = spm.SentencePieceProcessor()\n",
        "\n"
      ],
      "metadata": {
        "id": "d7RG29bQn0IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 불러와 인코딩, 디코딩 시험\n",
        "sp.load('mine.model')\n",
        "print(sp.encode(train_data[0], out_type=int))\n",
        "print(sp.DecodeIds([13702, 353, 7004, 394, 497, 930, 18833, 2368, 364, 2985, 1484, 1977, 19050, 1742, 167, 3834, 5689, 18747, 1587, 105, 18992, 19310, 19131, 18907, 1880]))\n"
      ],
      "metadata": {
        "id": "6j5loIVTt9CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, test_data 전부 Tokenize\n",
        "token_train_data = []\n",
        "for i in range(0,len(train_data)):\n",
        "  token_train_data.append(sp.encode(train_data[i], out_type = int))\n",
        "token_test_data = []\n",
        "for i in range(0,len(test_data))  :\n",
        "  token_test_data.append(sp.encode(test_data[i],out_type=int))\n",
        "\n",
        "#train_data_title, test_data_title 전부 Tokenize\n",
        "token_train_data_title = []\n",
        "for i in range(0,len(train_data_title)):\n",
        "  token_train_data_title.append(sp.encode(train_data_title[i], out_type=int))\n",
        "token_test_data_title = []\n",
        "for i in range(0,len(test_data_title)):\n",
        "  token_test_data_title.append(sp.encode(test_data_title[i], out_type=int))"
      ],
      "metadata": {
        "id": "I6rnpfXCwtqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문 - 벡터변환 (원-핫 인코딩)"
      ],
      "metadata": {
        "id": "1s1tjGR0wWgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 벡터변환 (원-핫 인코딩 방식 벡터변환으로 각 기사에 어떤 단어가 사용되었는지에 대한 정보를 담고 있다.)\n",
        "import numpy as np\n",
        "\n",
        "#행 : 각 기사들, 열 : 단어 시퀀스, 값 : 기사의 해당 단어 사용 여부\n",
        "def vectorize_squences(sequences, dimension=30000): #dimension은 sentencepiece의 vocab_size 와 같게\n",
        "  results = np.zeros((len(sequences),dimension))\n",
        "  print(results.shape)\n",
        "\n",
        "  #i번째 원소의 기사 sequence를 np.zero 각각의 줄에 그냥 다 넣기\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i,sequence] = 1\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_squences(token_train_data)\n",
        "x_test = vectorize_squences(token_test_data)\n",
        "x_train_title = vectorize_squences(token_train_data_title)\n",
        "x_test_title = vectorize_squences(token_test_data_title)"
      ],
      "metadata": {
        "id": "I9qHHVuWwRzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "라벨->벡터변환(원 핫 인코딩)\n",
        "\n",
        "\n",
        "1.   train_labels\n",
        "[3,4,3,6,1,2,6,3,7,3....]\n",
        "2.   one_hot_train_labels\n",
        "[0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,1],[1,0,0,0,0,0,0,0]\n"
      ],
      "metadata": {
        "id": "ngGMmXlt-L5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "라벨 값\n",
        "*   0-5은 processPattern 11-16\n",
        "*   6-9은 processPattern 21-24\n"
      ],
      "metadata": {
        "id": "_FTyeuK5CqDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#라벨 원-핫 인코딩 벡터변환\n",
        "def to_one_hot(labels, dimension=10):\n",
        "  results = np.zeros((len(labels),dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    if label <= 16:\n",
        "      results[i,label-11] = 1\n",
        "    elif label >=21:\n",
        "      results[i,label-15] = 1\n",
        "    else:\n",
        "      pass\n",
        "  return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "metadata": {
        "id": "oYLqcSnF_Tcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(one_hot_train_labels)\n",
        "print(one_hot_test_labels)"
      ],
      "metadata": {
        "id": "j1na8biRDXgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.모델 만들기\n",
        "\n",
        "\n",
        "입력 Layer 2개- 본문/제목\n",
        "은닉 Layer\n",
        "출력 Layer 10개\n"
      ],
      "metadata": {
        "id": "GWurdSxeBstI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 1:9 train:validation\n",
        "num1 = len(x_train)//10*3\n",
        "num2 = len(x_train_title)//10*3\n",
        "num3 = len(one_hot_train_labels)//10*3\n",
        "\n",
        "x_val = x_train[:num1]\n",
        "partial_x_train = x_train[num1:]\n",
        "\n",
        "x_val_title = x_train_title[:num2]\n",
        "partial_x_train_title = x_train[num2:]\n",
        "\n",
        "y_val = one_hot_train_labels[:num3]\n",
        "partial_y_train = one_hot_train_labels[num3:]\n"
      ],
      "metadata": {
        "id": "KJcXbb-I__3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num1, num2, num3)"
      ],
      "metadata": {
        "id": "daOFGWwGeRtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델 구성\n",
        "출력 클래스 개수 10개"
      ],
      "metadata": {
        "id": "M6G1C7L2CdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################다중입력 모델 만들어보기\n",
        "import keras\n",
        "import tensorflow\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "#본문, 타이틀 input\n",
        "title_input = keras.Input(shape=(30000,), name = 'title') #shape=(5000,) 은 1차원 배열 5000개를 받는다는 뜻\n",
        "content_input = keras.Input(shape=(30000,), name = 'content')\n",
        "\n",
        "x1 = layers.Dense(64, activation='relu')(title_input)\n",
        "x1 = layers.Dropout(0.5)(x1) #과적합 방지\n",
        "x1 = layers.Dense(64, activation='relu')(x1)\n",
        "\n",
        "x2 = layers.Dense(64, activation='relu')(content_input)\n",
        "x2 = layers.Dropout(0.5)(x2)\n",
        "x2 = layers.Dense(64, activation='relu')(x2)\n",
        "\n",
        "concatenated = keras.layers.concatenate([x1,x2])\n",
        "concatenated = layers.Dropout(0.1)(concatenated)\n",
        "\n",
        "output = layers.Dense(10, activation = 'softmax')(concatenated)\n",
        "\n",
        "model = models.Model(inputs = [title_input, content_input], outputs = output)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FGJoXo0ya7Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training accuracy 증가 & validation accuracy 감소 : 과대적합\n",
        "training data라는 악기에만 적응하고 validation이라는 새로운 악기는 연주하지 못하게 되는 현상\n",
        "\n",
        "\n",
        "1.   batch size 증가\n",
        "2.   항목 추가\n",
        "\n"
      ],
      "metadata": {
        "id": "2-N6nfayseRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([partial_x_train_title, partial_x_train],\n",
        "                    partial_y_train, #모델이 예측해야 하는 타켓 클래스로, 마지막 Dense Layer의 출력층과 연결되어 있음\n",
        "                    epochs=50,\n",
        "                    batch_size=200,\n",
        "                    validation_data=([x_val_title,x_val], y_val) )"
      ],
      "metadata": {
        "id": "rN9_1l51Dncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 구조"
      ],
      "metadata": {
        "id": "RIML_iT2hIcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "x81QNaYghHoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능 그래프"
      ],
      "metadata": {
        "id": "rIUXGpsthDvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 훈련 및 검증 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lZmUlnz3dtko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 시연"
      ],
      "metadata": {
        "id": "2T5HDabeZA7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로운 데이터로 테스트해보기\n",
        "본문->토큰화->벡터화->모델 분석 ->argmax 함수"
      ],
      "metadata": {
        "id": "y1O50FtdF-uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = '''다니엘 헤니, 14세 연하 아내♥에 진한 키스…거침없는 애정표현\" '''\n",
        "\n",
        "text = '''[스포츠조선닷컴 이게은기자] 배우 다니엘 헤니와 그의 아내 배우 루 쿠마가이가 달달한 케미를 뽐냈다.\n",
        "\n",
        "\n",
        "9일 루 쿠마가이는 다니엘 헤니와 함께 찍은 사진을 공개했다.\n",
        "\n",
        "사진 속 루 쿠마가이는 다니엘 헤니에게 꼬옥 안긴 채 애정을 드러냈고 두 사람은 입맞춤으로 더욱 행복해 보이는 투샷을 만들었다.\n",
        "\n",
        "맑은 하늘, 바다와 어우러져 마치 화보 같은 분위기를 풍겼다. 달달한 분위기가 보는 이들에게 부러움을 안겼다.\n",
        "\n",
        "한편 다니엘 헤니는 지난 10월 결혼 소식을 전했다. 당시 소속사는 \"(열애설) 당시 친구였던 두 사람은 이 일을 계기로 서서히 연인으로 발전했으며, 최근 양가 가족분들을 모시고 조용히 식을 올렸다. 사전에 소식을 전하지 못한 점 너른 양해 부탁드린다\"라고 밝혔다.\n",
        "\n",
        "루 쿠마가이는 다니엘 헤니보다 14세 연하로 미국에서 활동하고 있는 아시안계 배우이며 '9-1-1', '온리 더 브레이브', '굿 트러블', '라이언 핸슨 솔브스 크라임 온 텔레비전' 등에 출연했다.'''\n",
        "\n",
        "\n",
        "#토큰화->패딩->벡터화->입력\n",
        "token_title = sp.encode(title,out_type= int)\n",
        "token_text = sp.encode(text,out_type=int)\n",
        "\n",
        "\n",
        "padded_title = pad_sequences([token_title], maxlen=500) #pad_sequences 는 리스트를 받기 때문에 []해줌\n",
        "padded_text = pad_sequences([token_text], maxlen=500)\n",
        "\n",
        "vec_title = vectorize_squences(padded_title)\n",
        "vec_text = vectorize_squences(padded_text)\n",
        "\n",
        "pred1 = model.predict([vec_title,vec_text])\n",
        "\n",
        "np.argmax(pred1[0])"
      ],
      "metadata": {
        "id": "6bvo6NiLE-gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "#Sequential 모델 정의\n",
        "model = models.Sequential()\n",
        "#입력층 - 64개의 뉴런, relu 활성화 함수, 10000개의 입력데이터\n",
        "model.add(layers.Dense(64, activation='relu',input_shape=(5000,)))\n",
        "#은닉층\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "#출력층\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NSfWR-8Ve-6s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tswnqrlcmVnY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}